{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue Studio Taxi Demo\n",
    "\n",
    "In this demo we will load NYC taxi information into S3 and populate the Glue Data Catalog. Once the catalog is populated with the source tables we will use features in Glue Studio to visually create parquet formatted data denormalized into a curated data set.\n",
    "\n",
    "We will be using the Python boto3 library for parts of the demo and resources created from scripts executed in CloudFormation leveraging the CDK. To get started with this demo review the [README.md](README.md) file.\n",
    "\n",
    "You can find more information about the Python `boto3` library [here](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "lf = boto3.client('lakeformation')\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Outputs from the CloudFormation template\n",
    "\n",
    "The `GlueStudioDemoStack` needs to be executed before starting the demo. From this repo you can run the commands in the root directory below to get started if you just came to the notebook. To launch the CloudFormation script this requires the [AWS Cloud Development Kit](https://aws.amazon.com/cdk/)(CDK). The CDK synthizes CloudFormation templates written in `TypeScript` for this demo but other languages are supported. For more information on the CDK API go [here](https://docs.aws.amazon.com/cdk/api/latest/docs/aws-construct-library.html).\n",
    "\n",
    "From the root directory:\n",
    "\n",
    "``` bash\n",
    "npm install\n",
    "\n",
    "cdk deploy\n",
    "```\n",
    "\n",
    "Once this is deployed sucecssfully you will be able to get the `Outputs` from the CloudFormation stack. It creates the resources below:\n",
    "\n",
    "* S3 Bucket\n",
    "* IAM Role\n",
    "* Glue Database\n",
    "* Glue Crawler\n",
    "\n",
    "These resources will be used throughout the demo to store files, provide access to the data lake, and populate the catalog to be used by Glue Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cfn.describe_stacks(\n",
    "    StackName='GlueStudioDemoStack'\n",
    ")\n",
    "\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "for output in outputs:\n",
    "    if (output['OutputKey'] == 'DataLakeBucketName'):\n",
    "        bucket = output['OutputValue']\n",
    "    if (output['OutputKey'] == 'TaxiDatabase'):\n",
    "        database_name = output['OutputValue']\n",
    "    if (output['OutputKey'] == 'DataLakeRoleArn'):\n",
    "        role_arn = output['OutputValue']\n",
    "    if (output['OutputKey'] == 'TaxiDataCrawler'):\n",
    "        data_crawler = output['OutputValue']        \n",
    "        \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(outputs, columns=[\"OutputKey\", \"OutputValue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data files to S3\n",
    "\n",
    "Next, we will upload the `.csv` files located in the `data` folder to S3 to be used later in the demo. We are using a sample file from New York City Taxi and Limousine Commission (TLC) Trip Record Data dataset available on the [AWS Open Data Registry](https://registry.opendata.aws/nyc-tlc-trip-records-pds/).\n",
    "\n",
    "[s3.upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file) boto3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'yellow_tripdata_2020-06.csv'\n",
    "path = 'data'\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join('datalake', 'yellow', file_name)).upload_file(path + '/' + file_name)\n",
    "\n",
    "file_name = 'paymenttype.csv'\n",
    "path = 'data'\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join('datalake', 'paymenttype', file_name)).upload_file(path + '/' + file_name)\n",
    "\n",
    "file_name = 'ratecode.csv'\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join('datalake', 'ratecode', file_name)).upload_file(path + '/' + file_name)\n",
    "\n",
    "file_name = 'taxi_zone_lookup.csv'\n",
    "session.resource('s3').Bucket(bucket).Object(os.path.join('datalake', 'taxi_zone_lookup', file_name)).upload_file(path + '/' + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Taxi Demo database with S3 data from Glue Crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.start_crawler(Name=data_crawler)\n",
    "\n",
    "crawler_status = glue.get_crawler(Name=data_crawler)['Crawler']['State']\n",
    "\n",
    "while crawler_status not in ('READY'):\n",
    "    crawler_status = glue.get_crawler(Name=data_crawler)['Crawler']['State']\n",
    "    print(crawler_status)\n",
    "    time.sleep(30)\n",
    "    \n",
    "print('Crawler Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Crawler Results\n",
    "\n",
    "Now that we have crawled the taxi data, we want to look at the results of the crawl to see the tables that were created. We will call the [glue.get_tables](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_tables) function and load key fields into a pandas DataFrame for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(glue.get_tables(DatabaseName=database_name)['TableList'])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame(df, columns=[\"Name\", \"DatabaseName\", \"StorageDescriptor.Columns\", \"StorageDescriptor.Location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start with Glue Studio\n",
    "\n",
    "Run the cell below to take you to the Glue Studio console and use the instructions below to create the Glue Job visually. We will be joining multiple tables from the Glue Data Catalog into a single data set and save it to the S3 bucket created earlier in Parquet format. Later, we will use the job created with Glue Studio in a Glue Workflow to create and end-to-end ETL solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\"https://console.aws.amazon.com/gluestudio/home?region={0}#/\".format(region)])\n",
    "df.columns = ['Link']\n",
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\" target=\"_blank\">{}</a>'.format(val,val)\n",
    "\n",
    "df.style.hide_index().format(make_clickable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
